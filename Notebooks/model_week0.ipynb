{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling the events using dask library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.delayed as delayed\n",
    "import dask.bag as db\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into Dask dataframes\n",
    "df_data = dd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/data.csv/rating.csv\")\n",
    "df_raw = dd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/raw_data/data.csv\")\n",
    "df_traffic = dd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/traffic_data/traffic.csv\")\n",
    "df_domains = dd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/domains_location.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values\n",
    "missing_percentage = df_data.isnull().sum() / len(df_data) * 100\n",
    "print(missing_percentage.compute())  # Compute the result to get the actual percentages\n",
    "\n",
    "# Replace missing values in text columns with 'Unknown'\n",
    "df_data['author'] = df_data['author'].fillna('Unknown')\n",
    "df_data['description'] = df_data['description'].fillna('Not Available')\n",
    "df_data['url_to_image'] = df_data['url_to_image'].fillna('Not Available')\n",
    "df_data['category'] = df_data['category'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape[0].compute(), \"rows,\", df_data.shape[1], \"columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction/modelling using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove emoticons\n",
    "    text = re.sub(r':\\)|;\\)|:-\\)|:-\\(|:-D|:-\\(|:-\\)|:D|:P|:S|:\\||:O|:\\(|:\\-D|:\\-S', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to perform TF-IDF vectorization and keyword extraction\n",
    "def extract_keywords(df_data):\n",
    "    # Apply text preprocessing to 'content' column\n",
    "    df_data['clean_content'] = df_data['content'].apply(preprocess_text)\n",
    "    \n",
    "    # Perform TF-IDF vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_data['clean_content'])\n",
    "    \n",
    "    # Delayed function to compute TF-IDF matrix and feature names\n",
    "    @delayed\n",
    "    def compute_tfidf():\n",
    "        # Convert TF-IDF matrix to Dask array\n",
    "        tfidf_array = da.from_array(tfidf_matrix)\n",
    "        \n",
    "        # Get feature names (words)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        return tfidf_array, feature_names\n",
    "    \n",
    "    # Compute TF-IDF matrix and feature names\n",
    "    tfidf_array, feature_names = compute_tfidf().compute()\n",
    "    \n",
    "    # Create a DataFrame from TF-IDF array\n",
    "    tfidf_df = dd.from_dask_array(tfidf_array, columns=feature_names)\n",
    "    \n",
    "    # Identify top keywords\n",
    "    top_keywords = tfidf_df.sum().nlargest(10)\n",
    "    return top_keywords\n",
    "\n",
    "# Sample data preprocessing and keyword extraction\n",
    "top_keywords = extract_keywords(df_data)\n",
    "\n",
    "# Display top keywords\n",
    "print(\"Top Keywords:\", top_keywords.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in the 'title' column to strings only if they are convertible\n",
    "df_data['title'] = df_data.map_partitions(lambda df: df['title'].astype(str), meta=('title', 'str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between headline/title and news body for a subset of articles\n",
    "def calculate_similarity_subset(df_data, sample_size=1000):\n",
    "    # Sample subset of articles\n",
    "    df_sample = df_data.sample(frac=sample_size/len(df_data), random_state=42)\n",
    "    \n",
    "    # Apply text preprocessing to 'title' and 'content' columns\n",
    "    df_sample['clean_title'] = df_sample['title'].map_partitions(preprocess_text)\n",
    "    df_sample['clean_content'] = df_sample['content'].map_partitions(preprocess_text)\n",
    "    \n",
    "    # Delayed function to compute TF-IDF matrix and cosine similarity\n",
    "    @delayed\n",
    "    def compute_similarity(df_sample):\n",
    "        # Perform TF-IDF vectorization for title\n",
    "        tfidf_vectorizer_title = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix_title = tfidf_vectorizer_title.fit_transform(df_sample['clean_title'])\n",
    "        \n",
    "        # Perform TF-IDF vectorization for content\n",
    "        tfidf_vectorizer_content = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix_content = tfidf_vectorizer_content.fit_transform(df_sample['clean_content'])\n",
    "        \n",
    "        # Calculate cosine similarity between title and content vectors\n",
    "        similarity_scores = cosine_similarity(tfidf_matrix_title, tfidf_matrix_content)\n",
    "        return similarity_scores\n",
    "    \n",
    "    # Compute similarity scores for each partition\n",
    "    similarity_scores = compute_similarity(df_sample)\n",
    "    \n",
    "    # Aggregate similarity scores across partitions\n",
    "    overall_similarity = da.from_delayed(similarity_scores)\n",
    "    return overall_similarity.mean().compute()\n",
    "\n",
    "# Calculate similarity scores for a subset of articles\n",
    "overall_similarity_subset = calculate_similarity_subset(df_data)\n",
    "\n",
    "print(\"Overall similarity between keywords in headline/title and news body across sites (subset):\", overall_similarity_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
