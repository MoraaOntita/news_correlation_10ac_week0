{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import pycountry\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/data.csv/rating.csv\")\n",
    "df_raw = pd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/raw_data/data.csv\")\n",
    "df_traffic = pd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/traffic_data/traffic.csv\")\n",
    "df_domains = pd.read_csv(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/week0/Datasets/domains_location.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom Websites that have the largest count of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['source_name'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Count the number of news articles for each website\n",
    "website_article_counts = df_data['source_name'].value_counts()\n",
    "\n",
    "# Convert the series to a DataFrame for easier manipulation\n",
    "website_article_counts_df = pd.DataFrame(website_article_counts)\n",
    "website_article_counts_df.reset_index(inplace=True)\n",
    "website_article_counts_df.columns = ['Website', 'Article Count']\n",
    "\n",
    "# Sort the websites based on their article counts in descending order to find the top 10\n",
    "top_10_websites = website_article_counts_df.nlargest(10, 'Article Count')\n",
    "\n",
    "# Sort the websites based on their article counts in ascending order to find the bottom 10\n",
    "bottom_10_websites = website_article_counts_df.nsmallest(10, 'Article Count')\n",
    "\n",
    "print(\"Top 10 websites with the largest count of news articles:\")\n",
    "print(top_10_websites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBottom 10 websites with the smallest count of news articles:\")\n",
    "print(bottom_10_websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom 10 Websites with the highest numbers of visitors traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the DataFrame by GlobalRank in ascending order to get the top 10\n",
    "top_10_websites = df_traffic.sort_values(by='GlobalRank').head(10)\n",
    "\n",
    "# Sorting the DataFrame by GlobalRank in descending order to get the bottom 10\n",
    "bottom_10_websites = df_traffic.sort_values(by='GlobalRank', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Websites with the Highest Visitor Traffic:\")\n",
    "print(top_10_websites[['Domain', 'GlobalRank']])\n",
    "\n",
    "print(\"\\nBottom 10 Websites with the Lowest Visitor Traffic:\")\n",
    "print(bottom_10_websites[['Domain', 'GlobalRank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom 10 Countries with the highest number of news media organisations (represented by domains in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the DataFrame by 'location' and counting the number of unique domains for each country\n",
    "country_counts = df_domains.groupby('location')['SourceCommonName'].nunique().reset_index()\n",
    "\n",
    "# Sorting the DataFrame by the counts of unique domains in descending order to get the top 10\n",
    "top_10_countries = country_counts.sort_values(by='SourceCommonName', ascending=False).head(10)\n",
    "\n",
    "# Sorting the DataFrame by the counts of unique domains in ascending order to get the bottom 10\n",
    "bottom_10_countries = country_counts.sort_values(by='SourceCommonName').head(10)\n",
    "\n",
    "print(\"Top 10 Countries with the Highest Number of News Media Organizations:\")\n",
    "print(top_10_countries)\n",
    "\n",
    "print(\"\\nBottom 10 Countries with the Lowest Number of News Media Organizations:\")\n",
    "print(bottom_10_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom 10 Countries that have many articles written about them - the content of the news is about that country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract country mentions from text using spaCy\n",
    "def extract_countries(text):\n",
    "    doc = nlp(text)\n",
    "    countries = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return countries\n",
    "\n",
    "# Apply the function to extract country mentions from the content column\n",
    "df_data['country_mentions'] = df_data['content'].apply(extract_countries)\n",
    "\n",
    "# Flatten the list of country mentions\n",
    "country_mentions_flat = [country for sublist in df_data['country_mentions'] for country in sublist]\n",
    "\n",
    "# Count the occurrences of each country mention\n",
    "country_counts = pd.Series(country_mentions_flat).value_counts()\n",
    "\n",
    "# Display the top and bottom 10 countries mentioned in the articles\n",
    "top_10_countries = country_counts.head(10)\n",
    "bottom_10_countries = country_counts.tail(10)\n",
    "\n",
    "print(\"Top 10 countries mentioned in the articles:\")\n",
    "print(top_10_countries)\n",
    "\n",
    "print(\"\\nBottom 10 countries mentioned in the articles:\")\n",
    "print(bottom_10_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom Websites that reported (the news content) about Africa, US, China, EU, Russia, Ukraine, Middle East? Note that you will need to group countries together to form the African, EU, and Middle East continents/regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains['Country'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries, regions, and continents\n",
    "countries_list = [\n",
    "    'us', 'usa', 'united states', 'united states of america', 'america', 'united states of',\n",
    "    'china', 'prc', 'people\\'s republic of china',\n",
    "    'russia', 'russian federation', 'soviet union', 'ussr',\n",
    "    'ukraine', 'ukr', 'ukrainian',\n",
    "    'middle east', 'middle eastern', 'mideast',\n",
    "    'africa', 'african',\n",
    "    'north america', 'north american', 'na',  # For USA\n",
    "    'asia', 'asian', 'cn'  # For China\n",
    "]\n",
    "\n",
    "# Function to extract countries/regions from text\n",
    "def extract_countries(text):\n",
    "    doc = nlp(text)\n",
    "    countries = [ent.text.lower() for ent in doc.ents if ent.label_ == \"GPE\" and ent.text.lower() in countries_list]\n",
    "    return countries\n",
    "\n",
    "# Apply the function to extract countries from content column\n",
    "df_data['mentioned_countries'] = df_data['content'].apply(extract_countries)\n",
    "\n",
    "# Count mentions of each country/region\n",
    "df_data['mentioned_countries_count'] = df_data['mentioned_countries'].apply(len)\n",
    "\n",
    "# Group by source_name and sum the counts of mentions\n",
    "grouped_by_source = df_data.groupby('source_name')['mentioned_countries_count'].sum()\n",
    "\n",
    "# Sort the groups by counts of mentions\n",
    "sorted_sources = grouped_by_source.sort_values(ascending=False)\n",
    "\n",
    "# Extract top and bottom 10 sources\n",
    "top_10_sources = sorted_sources.head(10)\n",
    "bottom_10_sources = sorted_sources.tail(10)\n",
    "\n",
    "print(\"Top 10 sources reporting news about the regions:\")\n",
    "print(top_10_sources)\n",
    "print(\"\\nBottom 10 sources reporting news about the regions:\")\n",
    "print(bottom_10_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and Bottom 10 Websites with the highest count of positive, neutral, and negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['url'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    try:\n",
    "        parsed_uri = urlparse(url)\n",
    "        if parsed_uri.scheme == '' or parsed_uri.netloc == '':\n",
    "            return None\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        return domain\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract domain from the 'url' column\n",
    "df_data['domain'] = df_data['url'].apply(extract_domain)\n",
    "\n",
    "# Group the data by website domain and sentiment, then count the occurrences\n",
    "sentiment_counts = df_data.groupby(['domain', 'title_sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate total count of sentiments for each website\n",
    "sentiment_counts['Total'] = sentiment_counts.sum(axis=1)\n",
    "\n",
    "# Sort the websites based on total count of each sentiment\n",
    "top_bottom_website_sentiments = sentiment_counts.sort_values(by='Total', ascending=False)\n",
    "\n",
    "# Extract top and bottom 10 websites for each sentiment\n",
    "top_positive = top_bottom_website_sentiments.nlargest(10, 'Positive')['Positive']\n",
    "top_neutral = top_bottom_website_sentiments.nlargest(10, 'Neutral')['Neutral']\n",
    "top_negative = top_bottom_website_sentiments.nlargest(10, 'Negative')['Negative']\n",
    "\n",
    "bottom_positive = top_bottom_website_sentiments.nsmallest(10, 'Positive')['Positive']\n",
    "bottom_neutral = top_bottom_website_sentiments.nsmallest(10, 'Neutral')['Neutral']\n",
    "bottom_negative = top_bottom_website_sentiments.nsmallest(10, 'Negative')['Negative']\n",
    "\n",
    "print(\"Top 10 Websites with the highest count of positive sentiment:\")\n",
    "print(top_positive)\n",
    "print(\"\\nTop 10 Websites with the highest count of neutral sentiment:\")\n",
    "print(top_neutral)\n",
    "print(\"\\nTop 10 Websites with the highest count of negative sentiment:\")\n",
    "print(top_negative)\n",
    "\n",
    "print(\"\\nBottom 10 Websites with the lowest count of positive sentiment:\")\n",
    "print(bottom_positive)\n",
    "print(\"\\nBottom 10 Websites with the lowest count of neutral sentiment:\")\n",
    "print(bottom_neutral)\n",
    "print(\"\\nBottom 10 Websites with the lowest count of negative sentiment:\")\n",
    "print(bottom_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the impact of using mean/average and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by website domain and sentiment, then calculate mean sentiment counts\n",
    "mean_sentiment_counts = df_data.groupby('domain')['title_sentiment'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "# Calculate mean sentiment counts for each website\n",
    "mean_sentiment_counts['Mean_Total'] = mean_sentiment_counts.sum(axis=1)\n",
    "\n",
    "# Sort the websites based on mean total count of each sentiment\n",
    "mean_top_bottom_websites = mean_sentiment_counts.sort_values(by='Mean_Total', ascending=False)\n",
    "\n",
    "# Repeat the same process for median sentiment counts\n",
    "median_sentiment_counts = df_data.groupby('domain')['title_sentiment'].value_counts().unstack(fill_value=0)\n",
    "median_sentiment_counts['Median_Total'] = median_sentiment_counts.sum(axis=1)\n",
    "median_top_bottom_websites = median_sentiment_counts.sort_values(by='Median_Total', ascending=False)\n",
    "\n",
    "# Compare the top and bottom websites based on mean vs median sentiment counts for each sentiment\n",
    "for sentiment in ['Positive', 'Neutral', 'Negative']:\n",
    "    print(f\"Top 10 Websites with the highest count of {sentiment.lower()} sentiment (using Mean):\")\n",
    "    print(mean_top_bottom_websites[sentiment].nlargest(10))\n",
    "    print(f\"\\nTop 10 Websites with the highest count of {sentiment.lower()} sentiment (using Median):\")\n",
    "    print(median_top_bottom_websites[sentiment].nlargest(10))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(f\"Bottom 10 Websites with the lowest count of {sentiment.lower()} sentiment (using Mean):\")\n",
    "    print(mean_top_bottom_websites[sentiment].nsmallest(10))\n",
    "    print(f\"\\nBottom 10 Websites with the lowest count of {sentiment.lower()} sentiment (using Median):\")\n",
    "    print(median_top_bottom_websites[sentiment].nsmallest(10))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of Sentiments for a particular domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['domain'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and calculate the number of articles for each domain\n",
    "domain_article_counts = df_data['domain'].value_counts()\n",
    "\n",
    "# Select the top 10 domains by number of articles\n",
    "top_10_domains = domain_article_counts.nlargest(10).index\n",
    "\n",
    "# Filter df_data to include only articles from the top 10 domains\n",
    "df_top_10_domains = df_data[df_data['domain'].isin(top_10_domains)]\n",
    "\n",
    "# Calculate sentiment distribution for each domain\n",
    "domain_sentiment_distribution = df_top_10_domains.groupby(['domain', 'title_sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate global news sentiment distribution\n",
    "global_sentiment_distribution = df_data['title_sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the sentiment distributions for each domain and the global news sentiment distribution\n",
    "print(\"Sentiment Distribution for Top 10 Domains:\")\n",
    "print(domain_sentiment_distribution)\n",
    "\n",
    "print(\"\\nGlobal News Sentiment Distribution:\")\n",
    "print(global_sentiment_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar are the raw message lengths across sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data and create a copy to avoid the warning\n",
    "relevant_data = df_data[['source_name', 'content']].copy()\n",
    "\n",
    "# Calculate raw message lengths using .loc to set values explicitly\n",
    "relevant_data.loc[:, 'message_length'] = relevant_data['content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Group by Site and Calculate Descriptive Statistics\n",
    "stats_by_site = relevant_data.groupby('source_name')['message_length'].describe()\n",
    "\n",
    "# 4. Visualize the Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxplot = relevant_data.boxplot(column='message_length', by='source_name', patch_artist=True)\n",
    "plt.title('Distribution of Raw Message Lengths Across Sites')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Site')\n",
    "plt.ylabel('Message Length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, you can also plot histograms for better understanding of the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for site, data in relevant_data.groupby('source_name'):\n",
    "    plt.hist(data['message_length'], bins=50, alpha=0.5, label=site, density=True, range=(0, 2000))  \n",
    "plt.title('Distribution of Raw Message Lengths Across Sites')\n",
    "plt.xlabel('Message Length')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar are the number of words in the title across sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data and create a copy to avoid the warning\n",
    "relevant_data = df_data[['source_name', 'title']].copy()\n",
    "\n",
    "# Calculate the number of words in the title\n",
    "relevant_data['title_word_count'] = relevant_data['title'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot histograms with adjusted scale (increased number of bins and adjusted range)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for site, data in relevant_data.groupby('source_name'):\n",
    "    plt.hist(data['title_word_count'], bins=20, alpha=0.5, label=site, density=True)  \n",
    "plt.title('Distribution of Title Word Counts Across Sites')\n",
    "plt.xlabel('Number of Words in Title')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of frequent news reporting and sentiment to the website’s global ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of reports by each website\n",
    "report_counts = df_data['source_name'].value_counts().reset_index()\n",
    "report_counts.columns = ['source_name', 'report_count']\n",
    "\n",
    "# Convert sentiment strings to numerical values\n",
    "sentiment_mapping = {'Positive': 1, 'Negative': -1, 'Neutral': 0}\n",
    "df_data['title_sentiment_numeric'] = df_data['title_sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Aggregate average sentiment for each website\n",
    "avg_sentiment = df_data.groupby('source_name')['title_sentiment_numeric'].mean().reset_index()\n",
    "avg_sentiment.columns = ['source_name', 'avg_sentiment']\n",
    "\n",
    "# Aggregate global ranking for each website\n",
    "global_ranking = df_traffic.groupby('Domain')['GlobalRank'].mean().reset_index()\n",
    "global_ranking.columns = ['source_name', 'GlobalRank']\n",
    "\n",
    "# Merge aggregated data\n",
    "merged_data = pd.merge(report_counts, avg_sentiment, on='source_name', how='outer')\n",
    "merged_data = pd.merge(merged_data, global_ranking, on='source_name', how='outer')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_data['report_count'], merged_data['GlobalRank'], c=merged_data['avg_sentiment'], cmap='coolwarm')\n",
    "plt.colorbar(label='Average Sentiment')\n",
    "plt.xlabel('Total Number of Reports')\n",
    "plt.ylabel('Global Ranking')\n",
    "plt.title('Impact of News Reporting Frequency and Sentiment on Website Global Ranking')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values\n",
    "missing_percentage = (df_data.isnull().sum() / len(df_data)) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the source_id column as it contain 69% missing values\n",
    "df_data = df_data.drop(columns=['source_id'])\n",
    "\n",
    "# Replace missing values in text columns with 'Unknown'\n",
    "df_data['author'].fillna('Unknown', inplace=True)\n",
    "df_data['description'] = df_data['description'].fillna('Not Available')\n",
    "df_data['url_to_image'] = df_data['url_to_image'].fillna('Not Available')\n",
    "df_data['category'] = df_data['category'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain_location.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode imputation for the 'Country' column\n",
    "mode_country = df_domains['Country'].mode()[0]\n",
    "df_domains['Country'].fillna(mode_country, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains['Country'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction/modelling using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove emoticons\n",
    "    text = re.sub(r':\\)|;\\)|:-\\)|:-\\(|:-D|:-\\(|:-\\)|:D|:P|:S|:\\||:O|:\\(|:\\-D|:\\-S', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform TF-IDF vectorization and keyword extraction\n",
    "def extract_keywords(df_data):\n",
    "    # Apply text preprocessing to 'content' column\n",
    "    df_data['clean_content'] = df_data['content'].apply(preprocess_text)\n",
    "    \n",
    "    # Perform TF-IDF vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_data['clean_content'])\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame from TF-IDF matrix\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Identify top keywords\n",
    "    top_keywords = tfidf_df.sum().sort_values(ascending=False).head(10)\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "# Sample data preprocessing and keyword extraction\n",
    "top_keywords = extract_keywords(df_data)\n",
    "\n",
    "# Display top keywords\n",
    "print(\"Top Keywords:\", top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in the 'title' column to strings only if they are convertible\n",
    "df_data['title'] = df_data['title'].apply(lambda x: str(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['title'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of the 'title' column\n",
    "print(df_data['title'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between headline/title and news body for a subset of articles\n",
    "def calculate_similarity_subset(df_data, sample_size=1000):\n",
    "    # Sample subset of articles\n",
    "    df_sample = df_data.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Apply text preprocessing to 'title' and 'content' columns\n",
    "    df_sample['clean_title'] = df_sample['title'].apply(preprocess_text)\n",
    "    df_sample['clean_content'] = df_sample['content'].apply(preprocess_text)\n",
    "    \n",
    "    # Perform TF-IDF vectorization for title\n",
    "    tfidf_vectorizer_title = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix_title = tfidf_vectorizer_title.fit_transform(df_sample['clean_title'])\n",
    "    \n",
    "    # Perform TF-IDF vectorization for content\n",
    "    tfidf_vectorizer_content = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix_content = tfidf_vectorizer_content.fit_transform(df_sample['clean_content'])\n",
    "    \n",
    "    # Calculate cosine similarity between title and content vectors\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix_title, tfidf_matrix_content)\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Calculate similarity scores for a subset of articles\n",
    "similarity_scores_subset = calculate_similarity_subset(df_data)\n",
    "\n",
    "# Aggregate similarity scores across all articles\n",
    "overall_similarity_subset = similarity_scores_subset.mean()\n",
    "\n",
    "print(\"Overall similarity between keywords in headline/title and news body across sites (subset):\", overall_similarity_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform topic modeling using LDA\n",
    "def perform_topic_modeling(df_data, num_topics=5):\n",
    "    # Apply text preprocessing to 'content' column\n",
    "    df_data['clean_content'] = df_data['content'].apply(preprocess_text)\n",
    "    \n",
    "    # Create document-term matrix using CountVectorizer\n",
    "    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(df_data['clean_content'])\n",
    "    \n",
    "    # Fit LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(dtm)\n",
    "    \n",
    "    return lda_model, vectorizer\n",
    "\n",
    "# Perform topic modeling\n",
    "num_topics = 5  # You can adjust the number of topics as needed\n",
    "lda_model, vectorizer = perform_topic_modeling(df_data, num_topics)\n",
    "\n",
    "# Function to display top words for each topic\n",
    "def display_topics(model, vectorizer, num_top_words=10):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "# Display top words for each topic\n",
    "display_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Step 1: Text Preprocessing (assuming you have already preprocessed the text data)\n",
    "preprocessed_text = df_data['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Create Document-Term Matrix\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')  # Adjust max_features as needed\n",
    "dtm = vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Step 3: Apply LDA Model\n",
    "num_topics = 10  # Adjust the number of topics as needed\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Step 4: Interpret Topics\n",
    "# Get the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "num_top_words = 10  # Number of top words to display for each topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic {topic_idx+1}:\")\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing the title/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_topic(title, content):\n",
    "    categories = {\n",
    "        'Breaking News': ['breaking news', 'urgent news', 'emergency'],\n",
    "        'Politics': ['politics', 'government', 'election', 'president', 'political parties'],\n",
    "        'World News': ['world news', 'international news', 'global events'],\n",
    "        'Business/Finance': ['business', 'finance', 'economy', 'market', 'stock', 'investment', 'company', 'entrepreneur'],\n",
    "        'Technology': ['technology', 'innovation', 'computer', 'software'],\n",
    "        'Science': ['science', 'research', 'discovery', 'experiment', 'physics', 'biology', 'chemistry', 'astronomy'],\n",
    "        'Health': ['health', 'healthcare', 'wellness', 'medicine', 'nutrition', 'fitness', 'disease', 'pandemic'],\n",
    "        'Entertainment': ['movie', 'film', 'actor', 'actress', 'music', 'celebrity', 'entertainment', 'Hollywood'],\n",
    "        'Sports': ['football', 'soccer', 'basketball', 'tennis', 'cricket'],\n",
    "        'Environment': ['environment', 'climate change', 'sustainability', 'pollution', 'conservation', 'ecosystem', 'wildlife'],\n",
    "        'Crime': ['crime', 'law enforcement', 'legal issues', 'criminal activity'],\n",
    "        'Education': ['education', 'school', 'university', 'student', 'teacher', 'learning', 'curriculum', 'academic'],\n",
    "        'Weather': ['weather', 'forecast', 'temperature', 'meteorology', 'climate', 'storm', 'rain', 'snow'],\n",
    "        'Other': [] \n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize category to None\n",
    "    category = None\n",
    "    \n",
    "    # Check for matches in title\n",
    "    for cat, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in title.lower():\n",
    "                category = cat\n",
    "                break\n",
    "        if category:\n",
    "            break\n",
    "            \n",
    "    # If no match found in title, check content\n",
    "    if not category:\n",
    "        for cat, keywords in categories.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in content.lower():\n",
    "                    category = cat\n",
    "                    break\n",
    "            if category:\n",
    "                break\n",
    "    \n",
    "    # If category is still None, assign it as \"Others\"\n",
    "    if not category:\n",
    "        category = \"Others\"\n",
    "                \n",
    "    return category\n",
    "\n",
    "# Apply categorization function to create new column 'topic_category'\n",
    "df_data['topic_category'] = df_data.apply(lambda row: categorize_topic(row['title'], row['content']), axis=1)\n",
    "\n",
    "# Display the DataFrame with the new 'topic_category' column\n",
    "print(df_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing topic and trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most diverse website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['clean_content'] = df_data['content'].apply(preprocess_text)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data['clean_content'], df_data['topic_category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform TF-IDF vectorizer on training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train a classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Diversity Score\n",
    "# Calculate diversity score for each website\n",
    "df_data['topic_category'] = classifier.predict(tfidf_vectorizer.transform(df_data['clean_content']))\n",
    "\n",
    "# Group by website and count unique categories\n",
    "website_diversity = df_data.groupby('source_name')['topic_category'].nunique()\n",
    "\n",
    "# Step 3: Identify Websites with High Diversity\n",
    "most_diverse_websites = website_diversity.sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 most diverse websites:\")\n",
    "print(most_diverse_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze Trends\n",
    "# Convert 'published_at' column to datetime\n",
    "df_data['published_at'] = pd.to_datetime(df_data['published_at'], errors='coerce')\n",
    "\n",
    "# Extract year and month from 'published_at' column\n",
    "df_data['year_month'] = df_data['published_at'].dt.to_period('M')\n",
    "\n",
    "# Group by year_month and source_name, count unique categories\n",
    "diversity_trends = df_data.groupby(['year_month', 'source_name'])['topic_category'].nunique().unstack()\n",
    "\n",
    "# Step 5: Visualize Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=most_diverse_websites.index, y=most_diverse_websites.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Website')\n",
    "plt.ylabel('Number of Unique Categories')\n",
    "plt.title('Top 10 Most Diverse Websites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze topic trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date from 'published_at' column\n",
    "df_data['date'] = df_data['published_at'].dt.date\n",
    "\n",
    "# Group by date and topic_category, then count occurrences\n",
    "topic_counts = df_data.groupby(['date', 'topic_category']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the table to get counts of each topic on each date\n",
    "topic_counts_pivot = topic_counts.pivot(index='date', columns='topic_category', values='count').fillna(0)\n",
    "\n",
    "# Define a color palette with at least 14 distinct colors\n",
    "colors = sns.color_palette('tab20', n_colors=14)\n",
    "\n",
    "# Plotting with a custom color palette\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=topic_counts_pivot, markers='o', palette=colors, alpha=0.8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Topics')\n",
    "plt.title('Topic Trends Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observed Trends*\n",
    "\n",
    "It seems that alot of people read news for five continuous days then skip two days probably the Weekend then peaks up again from Monday. The most read news article is about business and politics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_freq = df_data['category'].value_counts()\n",
    "topic_category_freq = df_data['topic_category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_freq.plot(kind='bar', color='skyblue', alpha=0.7, label='Category')\n",
    "topic_category_freq.plot(kind='bar', color='orange', alpha=0.7, label='Topic Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Categories')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and vectorize using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_data['content'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 13  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "pipeline = make_pipeline(Normalizer(), kmeans)\n",
    "pipeline.fit(tfidf_matrix)\n",
    "clusters = kmeans.labels_ \n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = pipeline.predict(tfidf_matrix)\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df_data['cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sparse TF-IDF matrix\n",
    "sparse_tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Dimensionality Reduction for Visualization using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "reduced_features = svd.fit_transform(sparse_tfidf_matrix)\n",
    "\n",
    "# Visualize the Clusters\n",
    "df_visualize = pd.DataFrame(reduced_features, columns=['PC1', 'PC2'])\n",
    "df_visualize['cluster'] = clusters\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_visualize, x='PC1', y='PC2', hue='cluster', palette='bright', legend='full')\n",
    "plt.title('2D Scatter Plot of Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 events are covered in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News site that reported events the earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'published_at' to datetime format\n",
    "df_data['published_at'] = pd.to_datetime(df_data['published_at'])\n",
    "\n",
    "# Group by source name and find the earliest publication date for each source\n",
    "earliest_publication_dates = df_data.groupby('source_name')['published_at'].min()\n",
    "\n",
    "# Sort the news sites based on their earliest publication date\n",
    "earliest_publication_dates = earliest_publication_dates.sort_values()\n",
    "\n",
    "# Display the news sites and their earliest publication dates\n",
    "print(earliest_publication_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which events have the highest reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the occurrences of each unique event\n",
    "event_counts = df_data['title'].value_counts()\n",
    "\n",
    "# Sort the events based on their occurrence counts\n",
    "sorted_events = event_counts.sort_values(ascending=False)\n",
    "\n",
    "# Display the events with the highest reporting\n",
    "print(\"Events with the highest reporting:\")\n",
    "print(sorted_events.head(10))  # Adjust the number based on how many top events you want to see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between news sites reporting events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to include only articles related to events\n",
    "event_articles = df_data[df_data['content'].notna()]\n",
    "\n",
    "# Aggregate data by news source to get a summary of each source's reporting activities\n",
    "reporting_summary = event_articles.groupby('source_name').size().reset_index(name='event_count')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#let's use the frequency of reporting as the feature\n",
    "# reporting_summary = df_data.groupby('source_name').size().reset_index(name='event_count')\n",
    "\n",
    "# Scale the feature(s) to ensure they have the same influence on clustering\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(reporting_summary[['event_count']])\n",
    "\n",
    "# Choose the number of clusters (k)\n",
    "k = 3  # You may need to adjust this based on your data and desired number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "reporting_summary['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Analyze the resulting clusters\n",
    "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "reporting_summary['cluster_center'] = cluster_centers[reporting_summary['cluster']]\n",
    "\n",
    "# Print the cluster centers and the news sources in each cluster\n",
    "print(\"Cluster Centers:\")\n",
    "print(pd.DataFrame(cluster_centers, columns=['event_count']))\n",
    "\n",
    "print(\"\\nNews Sources in Each Cluster:\")\n",
    "for cluster_label, group in reporting_summary.groupby('cluster'):\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    print(group[['source_name', 'event_count']])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
